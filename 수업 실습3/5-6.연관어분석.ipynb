{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 동시출현빈도 기반"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 개의 단어가 주어진 문맥에서 서로 얼마나 연관되어 있는지\n",
    "- 대상어와 다른 단어들이 같은 문맥 내에서 동시에 출현한 횟수를 세는 방법\n",
    "- 두 단어가 같은 문맥 내에서 함께 나타나는 빈도가 높을수록 강한 연관관계가 성립한다는 가정\n",
    "- 한 문장에서 한 단어가 여러번 중복되어 나타날 경우 1회로 계산하거나 n회로 계산, 1회로 계산하는 것이 일반적인 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "# 긍정리뷰 100개 로딩\n",
    "pos_review=(glob.glob(\"c:/vscode/data/imdb/train/pos/*.txt\"))[0:100]\n",
    "lines_pos=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except :\n",
    "        continue\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+') #알파벳, 숫자, _\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#동시출현 단어 계산\n",
    "count = {} #동시출현 빈도가 저장될 dict\n",
    "for line in lines_pos:\n",
    "    words = line.lower()\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "\n",
    "    #불용어 제거, 불용어에 br 추가\n",
    "    stopped_tokens = [i for i in list(set(tokens)) \n",
    "                        if i not in stop_words +[\"br\"]]\n",
    "    \n",
    "    #글자수가 1인 단어 제외\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]\n",
    "    for i, a in enumerate(stopped_tokens2):\n",
    "        for b in stopped_tokens2[i+1: ]:\n",
    "            if a>b:\n",
    "                count[b, a] = count.get((b, a),0) + 1\n",
    "            else :\n",
    "                count[a, b] = count.get((a, b),0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#딕셔너리로부터 데이터프레임 생성\n",
    "df = pd.DataFrame.from_dict(count, orient='index')\n",
    "\n",
    "#리스트 구성\n",
    "list1=[]\n",
    "for i in range(len(df)):\n",
    "    list1.append([df.index[i][0], df.index[i][1], df[0][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term1</th>\n",
       "      <th>term2</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movie</td>\n",
       "      <td>one</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>film</td>\n",
       "      <td>story</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie</td>\n",
       "      <td>story</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>film</td>\n",
       "      <td>movie</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one</td>\n",
       "      <td>story</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good</td>\n",
       "      <td>movie</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>film</td>\n",
       "      <td>one</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>movie</td>\n",
       "      <td>see</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>one</td>\n",
       "      <td>see</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>film</td>\n",
       "      <td>like</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>like</td>\n",
       "      <td>movie</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>film</td>\n",
       "      <td>good</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good</td>\n",
       "      <td>story</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>great</td>\n",
       "      <td>movie</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good</td>\n",
       "      <td>one</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>movie</td>\n",
       "      <td>much</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>film</td>\n",
       "      <td>well</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>movie</td>\n",
       "      <td>time</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>film</td>\n",
       "      <td>time</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>film</td>\n",
       "      <td>see</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    term1  term2  freq\n",
       "0   movie    one    41\n",
       "1    film  story    41\n",
       "2   movie  story    35\n",
       "3    film  movie    35\n",
       "4     one  story    33\n",
       "5    good  movie    32\n",
       "6    film    one    31\n",
       "7   movie    see    30\n",
       "8     one    see    27\n",
       "9    film   like    27\n",
       "10   like  movie    26\n",
       "11   film   good    26\n",
       "12   good  story    26\n",
       "13  great  movie    26\n",
       "14   good    one    25\n",
       "15  movie   much    25\n",
       "16   film   well    25\n",
       "17  movie   time    25\n",
       "18   film   time    25\n",
       "19   film    see    24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(list1, columns=[\"term1\",\"term2\",\"freq\"])\n",
    "df3 = df2.sort_values(by=['freq'],ascending=False)\n",
    "df3_pos = df3.reset_index(drop=True)\n",
    "\n",
    "#동시출현 단어 페어 빈도 상위 20개 출력\n",
    "df3_pos.head(20)\n",
    "#film과 story 총 41회 동시에 출현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "# 부정리뷰 100개 로딩\n",
    "pos_review=(glob.glob(\"c:/vscode/data/imdb/train/neg/*.txt\"))[0:100]\n",
    "lines_neg=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_neg.append(temp)\n",
    "        f.close()\n",
    "    except :\n",
    "        continue\n",
    "len(lines_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+') #알파벳, 숫자, _\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#동시출현 단어 계산\n",
    "count = {} #동시출현 빈도가 저장될 dict\n",
    "for line in lines_neg:\n",
    "    words = line.lower()\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "\n",
    "    #불용어 제거, 불용어에 br 추가\n",
    "    stopped_tokens = [i for i in list(set(tokens)) \n",
    "                        if i not in stop_words +[\"br\"]]\n",
    "    \n",
    "    #글자수가 1인 단어 제외\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]\n",
    "    for i, a in enumerate(stopped_tokens2):\n",
    "        for b in stopped_tokens2[i+1: ]:\n",
    "            if a>b:\n",
    "                count[b, a] = count.get((b, a),0) + 1\n",
    "            else :\n",
    "                count[a, b] = count.get((a, b),0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term1</th>\n",
       "      <th>term2</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film</td>\n",
       "      <td>movie</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>like</td>\n",
       "      <td>movie</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie</td>\n",
       "      <td>one</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>film</td>\n",
       "      <td>one</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>like</td>\n",
       "      <td>one</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>even</td>\n",
       "      <td>movie</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>good</td>\n",
       "      <td>movie</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>even</td>\n",
       "      <td>like</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>good</td>\n",
       "      <td>one</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>film</td>\n",
       "      <td>like</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>film</td>\n",
       "      <td>good</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good</td>\n",
       "      <td>like</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>movie</td>\n",
       "      <td>would</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>movie</td>\n",
       "      <td>much</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bad</td>\n",
       "      <td>one</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>movie</td>\n",
       "      <td>really</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>film</td>\n",
       "      <td>really</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>film</td>\n",
       "      <td>would</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>even</td>\n",
       "      <td>one</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bad</td>\n",
       "      <td>movie</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    term1   term2  freq\n",
       "0    film   movie    42\n",
       "1    like   movie    40\n",
       "2   movie     one    38\n",
       "3    film     one    35\n",
       "4    like     one    33\n",
       "5    even   movie    32\n",
       "6    good   movie    32\n",
       "7    even    like    31\n",
       "8    good     one    30\n",
       "9    film    like    29\n",
       "10   film    good    28\n",
       "11   good    like    27\n",
       "12  movie   would    27\n",
       "13  movie    much    26\n",
       "14    bad     one    25\n",
       "15  movie  really    25\n",
       "16   film  really    25\n",
       "17   film   would    25\n",
       "18   even     one    25\n",
       "19    bad   movie    25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#딕셔너리로부터 데이터프레임 생성\n",
    "df = pd.DataFrame.from_dict(count, orient='index')\n",
    "\n",
    "#리스트 구성\n",
    "list1=[]\n",
    "for i in range(len(df)):\n",
    "    list1.append([df.index[i][0], df.index[i][1], df[0][i]])\n",
    "\n",
    "df2 = pd.DataFrame(list1, columns=[\"term1\",\"term2\",\"freq\"])\n",
    "df3 = df2.sort_values(by=['freq'],ascending=False)\n",
    "df3_neg = df3.reset_index(drop=True)\n",
    "\n",
    "#동시출현 단어 페어 빈도 상위 20개 출력\n",
    "df3_neg.head(20)\n",
    "#film과 movie가 총 42회 동시에 출현"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 통계적 가중치 기반"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 통계적으로 가중치를 구한 후 두 단어 간의 유사도를 단어간의 연관도로 적용하는 방법\n",
    "- 1.단어마다 가중치를 할당해야 함(출현빈도, tf-idf 등으로 계산)\n",
    "- 2.단어간의 유사도 계산(cosine similarity 등의 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3989)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#TF-IDF 가중치 할당\n",
    "vec = TfidfVectorizer(stop_words=stop_words)\n",
    "vector_lines_pos = vec.fit_transform(lines_pos)\n",
    "A=vector_lines_pos.toarray()\n",
    "print(A.shape)\n",
    "# print(A)\n",
    "#x축 단어, y축 문서\n",
    "#현재 상태는 100개의 문서의 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3989, 100)\n"
     ]
    }
   ],
   "source": [
    "# 단어간의 유사도를 구하는 것이 목적이므로\n",
    "# 단어-문서 행렬로 변경\n",
    "# x축 문서, y축 단어\n",
    "A=A.transpose()\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t0.7\n",
      "  (2, 2)\t1.5\n",
      "[[0.5 0.  0. ]\n",
      " [0.  1.  0. ]\n",
      " [0.7 0.  1.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "#밀집행렬(dense array)\n",
    "a=np.array([[0.5,0,0],[0,1,0],[0.7,0,1.5]])\n",
    "\n",
    "#밀집행렬을 희소행렬(sparse array)로 변환\n",
    "#밀집행렬의 단점: 0이 많을 경우 메모리 낭비가 될 수 있음\n",
    "#희소행렬은 0이 아닌 값들의 위치와 값만 기록하여 메모리를 절약하는 방식\n",
    "b=sparse.csr_matrix(a)\n",
    "print(b)\n",
    "# (0,0) 0.5 => 인덱스 0,0에 값 0.5\n",
    "# (1,1) 1 => 인덱스 1,1에 값 1\n",
    "# (2,0) 0.7 => 인덱스 2,0에 값 0.7\n",
    "# (2,2) 1 => 인덱스 2,2에 값 1.5\n",
    "c=b.toarray() #희소행렬을 밀집행렬로 변환\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'friday'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "A_sparse = sparse.csr_matrix(A)\n",
    "\n",
    "#코사인 유사도 계산\n",
    "similarities_sparse = cosine_similarity(A_sparse,dense_output=False)\n",
    "\n",
    "# todok() 행렬을 딕셔너리 형태로 변환\n",
    "list(similarities_sparse.todok().items())[35000:35010]\n",
    "#list(similarities_sparse.todok().items())[-10:]\n",
    "#단어 이름이 아닌 인덱스 형태로 출력됨\n",
    "# (1469,108), 0.37 1469 단어와 108 단어의 유사도는 37%\n",
    "vec.get_feature_names_out()[1469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2613, 3250)</td>\n",
       "      <td>0.499980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(3250, 2613)</td>\n",
       "      <td>0.499980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2545, 1188)</td>\n",
       "      <td>0.499979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1188, 2545)</td>\n",
       "      <td>0.499979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(1767, 2543)</td>\n",
       "      <td>0.499965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(2543, 1767)</td>\n",
       "      <td>0.499965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(1074, 250)</td>\n",
       "      <td>0.499935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(250, 1074)</td>\n",
       "      <td>0.499935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(2322, 525)</td>\n",
       "      <td>0.499934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(525, 2322)</td>\n",
       "      <td>0.499934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words    weight\n",
       "0  (2613, 3250)  0.499980\n",
       "1  (3250, 2613)  0.499980\n",
       "2  (2545, 1188)  0.499979\n",
       "3  (1188, 2545)  0.499979\n",
       "4  (1767, 2543)  0.499965\n",
       "5  (2543, 1767)  0.499965\n",
       "6   (1074, 250)  0.499935\n",
       "7   (250, 1074)  0.499935\n",
       "8   (2322, 525)  0.499934\n",
       "9   (525, 2322)  0.499934"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#위의 결과값을 데이터프레임으로 출력\n",
    "df=pd.DataFrame(list(similarities_sparse.todok().items()),columns=[\"words\",\"weight\"])\n",
    "df2=df.sort_values(by=['weight'],ascending=False)\n",
    "df2=df2.reset_index(drop=True)\n",
    "\n",
    "#단어 자신끼리의 짝은 1이 되므로 1보다 작은 항목들만 출력시킴\n",
    "df3=df2.loc[np.round(df2['weight']) < 1]\n",
    "df3=df3.reset_index(drop=True)\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physically,society=>0.50\n",
      "society,physically=>0.50\n",
      "past,essence=>0.50\n",
      "essence,past=>0.50\n",
      "iceberg,passengers=>0.50\n",
      "passengers,iceberg=>0.50\n",
      "dying,art=>0.50\n",
      "art,dying=>0.50\n",
      "move,care=>0.50\n",
      "care,move=>0.50\n",
      "horner,long=>0.50\n",
      "long,horner=>0.50\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(df3.iterrows()):\n",
    "    a=vec.get_feature_names_out()[row[1][0][0]]\n",
    "    b=vec.get_feature_names_out()[row[1][0][1]]\n",
    "    print(f'{a},{b}=>{row[1][1]:.2f}')    \n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> word2vec (워드임베딩)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어의 의미는 그 단어 주변 단어의 분포로 이해될 수 있다\n",
    "- 단어의 의미는 단어 벡터 안에 인코딩될 수 있다.\n",
    "- 단순히 출현횟수만을 고려하는 것이 아닌 단어 위치, 순서도 고려하는 방법\n",
    "- CBOW 주변 단어로 중심 단어를 예측하는 방법\n",
    "- Skip-gram 중심 단어로 주변 단어를 예측하는 방법(최근에 더 많이 사용되는 방법)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9413078"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "#단어 추출\n",
    "text=[]\n",
    "for line in lines_pos:\n",
    "    words = line.lower()\n",
    "    tokens = tokenizer.tokenize(words)\n",
    "    stopped_tokens = [i for i in list(set(tokens)) \n",
    "                        if i not in stop_words+[\"br\"]]\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)>1]\n",
    "    text.append(stopped_tokens2)\n",
    "    \n",
    "# word2vec 모형 생성 , sg=1 skip-gram을 적용, \n",
    "# window=2 중심 단어로부터 좌우 2개의 단어까지 학습에 적용\n",
    "# min_count=3 최소 3회 이상 출현한 단어들을 대상으로 학습\n",
    "model = Word2Vec(text, vector_size=10, sg=1, window=2, min_count=3)\n",
    "\n",
    "# 두 단어의 유사도 계산\n",
    "model.wv.similarity('film', 'movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('directed', 0.892633855342865),\n",
       " ('near', 0.8925931453704834),\n",
       " ('needed', 0.886659562587738),\n",
       " ('like', 0.8846356272697449),\n",
       " ('school', 0.8817977905273438)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good과 가장 유사한 단어 5개\n",
    "model.wv.most_similar(\"good\",topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델에 저장된 단어의 갯수\n",
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델에 저장된 단어 텍스트\n",
    "model.wv.index_to_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16575187, -0.13935485,  0.30233198,  0.09304485,  0.08483201,\n",
       "       -0.02816552,  0.24418698,  0.23198113, -0.23904102, -0.25841346],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#단어에 해당하는 벡터값\n",
    "model.wv.vectors[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
