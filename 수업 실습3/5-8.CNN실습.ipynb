{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 합성곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "w = np.array([2, 1, 5, 3])  # 가중치, 필터, 커널\n",
    "x = np.array([2, 8, 3, 7, 1, 2, 0, 4, 5])\n",
    "\n",
    "w_r = np.flip(w)    #w 배열을 뒤집어서 출력\n",
    "print(w_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#합성곱 계산\n",
    "# w_r을 x의 왼쪽 자리에 맞추고 각 인덱스마다 곱한 후 더함\n",
    "# 2x3 + 8x5 + 3x1 + 7x2 = 63\n",
    "# w_r을 오른쪽으로 한자리 shift하여 곱셈\n",
    "for i in range(6):\n",
    "    print(np.dot(x[i:i+4], w_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사이파이에서 제공하는 합성곱 함수\n",
    "#w를 뒤집어서 곱하는 방식\n",
    "from scipy.signal import convolve\n",
    "# valid - 원본 배열에 패딩을 추가하지 않는 방식\n",
    "# 원본 이미지가 4x4인 경우 결과물이 3x3으로 줄어드는 방식\n",
    "convolve(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#교차상관 - w를 뒤집지 않고 곱하는 방식\n",
    "# '합성곱 신경망'에서는 w를 뒤집지 않고 그대로 곱하는 '교차상관' 방식을 사용함\n",
    "# 초기 가중치값은 랜덤으로 만들어지므로 뒤집어서 곱하는 것과 \n",
    "#뒤집지 않고 곱하는 것이 큰 의미가 없음\n",
    "# 정확히 표현하면 교차상관이지만 합성곱 신경망이라는 이름을 관례적으로 사용하고 있음\n",
    "from scipy.signal import correlate\n",
    "correlate(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full 패딩 - 제로패딩을 한 후 연산을 하게 되면 \n",
    "#원본 배열의 모든 원소가 연산에 동일하게 참여하게 됨\n",
    "correlate(x, w, mode='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#출력 배열의 길이가 원본 배열의 길이와 같아지도록 제로 패딩을 추가하는 방식\n",
    "#합성곱 신경망에서 많이 사용하는 방식\n",
    "correlate(x, w, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2차원 배열에 대한 합성곱 계산\n",
    "from scipy.signal import correlate2d\n",
    "x = np.array([[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9]])\n",
    "w = np.array([[2, 0],\n",
    "            [0, 0]])\n",
    "correlate2d(x, w, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#제로패딩을 하여 원본과 같은 사이즈로 출력되도록 함\n",
    "correlate2d(x, w, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텐서플로에서 지원하는 합성곱 함수\n",
    "import tensorflow as tf\n",
    "\n",
    "#4차원 배열을 사용해야 함\n",
    "x = np.array([[1, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9]])\n",
    "with tf.device('/CPU:0'):\n",
    "    # 입력값: reshape(batch, height, width, channel)\n",
    "    x_4d = x.astype(np.float).reshape(1, 3, 3, 1) #실수형으로 입력해야 함\n",
    "    # 필터(가중치) reshape(height,width,channel,가중치의개수)\n",
    "    w_4d = w.reshape(2, 2, 1, 1)\n",
    "    #SAME 대문자로 작성해야 함\n",
    "    c_out = tf.nn.conv2d(x_4d, w_4d, strides=1, padding='SAME')\n",
    "    # 텐서를 넘파이 배열로 변환\n",
    "    print(c_out.numpy().reshape(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#맥스풀링\n",
    "# 입력값: reshape(샘플수, height, width, channel)\n",
    "x = np.array([[1, 2, 3, 4],\n",
    "            [5, 6, 7, 8],\n",
    "            [9, 10, 11, 12],\n",
    "            [13, 14, 15, 16]])\n",
    "x = x.reshape(1, 4, 4, 1)\n",
    "# ksize 커널사이즈 2x2, strides 이동간격\n",
    "with tf.device('/CPU:0'):\n",
    "    p_out=tf.nn.max_pool2d(x,ksize=2,strides=2,padding='SAME')\n",
    "    print(p_out.numpy().reshape(2,2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 한글텍스트 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 부족할 때\n",
    "# 메모리 필요한만큼만 사용하는 옵션\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('c:/vscode/data/text/ratings_train.csv',encoding='ms949')\n",
    "test_data = pd.read_csv('c:/vscode/data/text/ratings_test.csv',encoding='ms949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjoeun\\AppData\\Local\\Temp\\ipykernel_6120\\2266659944.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "C:\\Users\\tjoeun\\AppData\\Local\\Temp\\ipykernel_6120\\2266659944.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train_data['document'] = train_data['document'].str.replace('^ +', \"\")\n"
     ]
    }
   ],
   "source": [
    "#중복값 제거\n",
    "import numpy as np\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "# Null 값이 존재하는 행 제거\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "#특수문자,기호 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "#공백 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\")\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "#null 샘플 제거\n",
    "train_data = train_data.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjoeun\\AppData\\Local\\Temp\\ipykernel_6120\\4291877067.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
      "C:\\Users\\tjoeun\\AppData\\Local\\Temp\\ipykernel_6120\\4291877067.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test_data['document'] = test_data['document'].str.replace('^ +', \"\")\n"
     ]
    }
   ],
   "source": [
    "# test_data에도 동일한 과정 적용\n",
    "test_data.drop_duplicates(subset = ['document'],inplace=True)\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\")\n",
    "test_data['document'].replace('', np.nan, inplace=True)\n",
    "test_data = test_data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 사전\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘',\n",
    "            '걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "#형태소 분석\n",
    "#for sentence in train_data['document']:\n",
    "for sentence in train_data['document'][:10000]:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X \n",
    "            if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터에 대한 토큰화\n",
    "X_test = []\n",
    "#for sentence in test_data['document']:\n",
    "for sentence in test_data['document'][:10000]:\n",
    "    temp_X = okt.morphs(sentence, stem=True)\n",
    "    temp_X = [word for word in temp_X \n",
    "            if not word in stopwords]\n",
    "    X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# 정수 인코딩\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "#print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12445\n",
      "8200\n",
      "단어 집합의 크기 : 4246\n"
     ]
    }
   ],
   "source": [
    "# 출현빈도가 3회 미만인 단어들\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어수\n",
    "rare_cnt = 0\n",
    "total_freq = 0\n",
    "rare_freq = 0\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "print(total_cnt) #단어집합 크기\n",
    "print(rare_cnt) #희귀단어수\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print('단어 집합의 크기 :',vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩\n",
    "#텍스트를 숫자 시퀀스로 변환\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.24000000000001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(train_data['label'][:10000])\n",
    "y_test = np.array(test_data['label'][:10000])\n",
    "\n",
    "max_len = max(len(l) for l in X_train) #리뷰의 최대 길이\n",
    "\n",
    "cnt = 0\n",
    "for s in X_train:\n",
    "    if(len(s) <= 30):\n",
    "        cnt = cnt + 1\n",
    "#최대 길이 이하인 샘플의 비율\n",
    "(cnt / len(X_train))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Dense, Flatten, MaxPooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length = max_len))\n",
    "model.add(Conv1D(filters = 64, kernel_size = 5, \n",
    "                padding = 'same',activation = 'relu', strides = 1))\n",
    "model.add(Conv1D(filters = 32, kernel_size = 4, \n",
    "                padding = 'same',activation = 'relu', strides = 1))\n",
    "model.add(Conv1D(filters = 16, kernel_size = 3, \n",
    "                padding = 'same',activation = 'relu', strides = 1))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', \n",
    "            optimizer ='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 53, 100)           424600    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 53, 64)            32064     \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 53, 32)            8224      \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 53, 16)            1552      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 10, 16)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 160)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 466,601\n",
      "Trainable params: 466,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "120/125 [===========================>..] - ETA: 0s - loss: 0.6163 - acc: 0.6236\n",
      "Epoch 1: val_acc improved from -inf to 0.77500, saving model to CNN_model.h5\n",
      "125/125 [==============================] - 7s 11ms/step - loss: 0.6087 - acc: 0.6305 - val_loss: 0.4744 - val_acc: 0.7750\n",
      "Epoch 2/10\n",
      "122/125 [============================>.] - ETA: 0s - loss: 0.3464 - acc: 0.8513\n",
      "Epoch 2: val_acc improved from 0.77500 to 0.79150, saving model to CNN_model.h5\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 0.3472 - acc: 0.8505 - val_loss: 0.4455 - val_acc: 0.7915\n",
      "Epoch 3/10\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9094\n",
      "Epoch 3: val_acc did not improve from 0.79150\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.2255 - acc: 0.9093 - val_loss: 0.5288 - val_acc: 0.7805\n",
      "Epoch 4/10\n",
      "121/125 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.9419\n",
      "Epoch 4: val_acc did not improve from 0.79150\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.1490 - acc: 0.9413 - val_loss: 0.6902 - val_acc: 0.7805\n",
      "Epoch 5/10\n",
      "117/125 [===========================>..] - ETA: 0s - loss: 0.0937 - acc: 0.9645\n",
      "Epoch 5: val_acc did not improve from 0.79150\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0972 - acc: 0.9638 - val_loss: 0.9420 - val_acc: 0.7730\n",
      "Epoch 6/10\n",
      "121/125 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9729\n",
      "Epoch 6: val_acc did not improve from 0.79150\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0707 - acc: 0.9729 - val_loss: 1.0726 - val_acc: 0.7705\n",
      "Epoch 7/10\n",
      "121/125 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9796\n",
      "Epoch 7: val_acc did not improve from 0.79150\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0507 - acc: 0.9793 - val_loss: 1.2429 - val_acc: 0.7670\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12dae597b50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
    "mc = ModelCheckpoint('CNN_model.h5', monitor='val_acc', \n",
    "                    mode='max', verbose=1, save_best_only=True)\n",
    "model.fit(X_train, y_train, batch_size = 64, epochs=10, \n",
    "        validation_split=0.2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4379 - acc: 0.7993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43785560131073, 0.7993000149726868]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = load_model('CNN_model.h5')\n",
    "loaded_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_predict(new_sentence):\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence \n",
    "                    if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer.texts_to_sequences([new_sentence]) #정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(model.predict(pad_new)) # 예측\n",
    "    if(score > 0.5):\n",
    "        print(f\"{score * 100:.2f}% 확률로 긍정 리뷰입니다.\\n\")\n",
    "    else:\n",
    "        print(f\"{(1 - score) * 100:.2f}% 확률로 부정 리뷰입니다.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 148ms/step\n",
      "84.27% 확률로 긍정 리뷰입니다.\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "99.98% 확률로 부정 리뷰입니다.\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "99.82% 확률로 긍정 리뷰입니다.\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "100.00% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_predict('연기는 잔잔하게 볼 만 합니다')\n",
    "review_predict('영화의 주제가 뭔지 모르겠음')\n",
    "review_predict('익살스런 연기가 돋보였던 영화')\n",
    "review_predict('기대보다는 스토리가 큰 감흥은 없습니다')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
